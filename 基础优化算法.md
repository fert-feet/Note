# 基础优化算法
## 梯度下降
- 挑选一个初始值 $w_0$（一般随机）
- 重复迭代参数 $t = 1,2,3$，使用公式不断更新 $w_0$ ，使其接近最优解
  $w_t = w_{t-1} - \eta {\frac {\partial\ell} {\partial w_{t-1}}}$
  其中 $\ell$ 是损失函数，$\eta$ 是学习率，公式的直观解释就是，$w$ 沿着随时函数**负梯度**方向（下降最快的方向）前进，$\eta$ 控制一次走多远 
- 沿梯度方向将增加损失函数值
- 学习率：步长的超参数（人为指定），不能太大或太小，太大就要折回来（振荡），太小就走不到下降的地方

![image-20240503115335262](https://s2.loli.net/2024/05/03/iJWmystYquewnCg.png)

### 小批量梯度下降（深度学习默认的学习算法）
- 在整个训练集上算梯度太贵，耗时较长
- 随机采样 $b$ 个样本 $i_1,i_2,...,i_b$ 来近似损失
  $\frac 1 b \sum_{i \in I_b}{\ell(x_i,y_i,w)}$
- b 是批量大小，也是重要的超参数，太大内存消耗增加浪费，太小不适合最大利用计算资源