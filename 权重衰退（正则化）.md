# 权重衰退（正则化）

## 简介

权重衰退（Weight Decay）是一种正则化技术，用于防止机器学习模型（尤其是神经网络）过拟合训练数据。过拟合发生在模型在训练数据上表现非常好，但在新数据（测试数据）上表现不佳，即模型没有很好的泛化能力。权重衰退通过向损失函数中添加一个正则化项来限制模型的复杂度，从而减小过拟合的风险。简而言之就是防止过拟合。

正则化会让拟合曲线更加平滑（由蓝线变为更合理展示数据趋势的绿线），防止过拟合

<img src="https://s2.loli.net/2024/06/03/5maQNSfnK2kq3bI.png" alt="image-20240603170508663" style="zoom: 80%;" />

## 原理

### 使用均方范数作为限制 

- 限制参数选择范围控制模型容量：最小化损失函数的时候加入对 w 的限制

  $min ~ \ell(w,b) ~ subject~to~||w||^2 <= \theta$

- 总的损失函数应该是：(一般情况下)，这两个公式应该是等价的，其中 $\lambda$ 是个超参数，新加的这个函数叫罚函数

  $L_{total}=L_{min}+{\frac \lambda 2}{ \sum_i{w_i}^2}$

- 通常不限制 b 

- 小的 $\theta$ 意味着更强的正则项，等价于更大的 $\lambda$

### 正则化对最优解的影响图像演示

图中公式：

- $w^* = arg~min~\ell(w,b) + {\frac \lambda 2}{ \sum_i{w_i}^2}$

- $\tilde{w}^* = arg~min~\ell(w,b) $

对于图像的直观理解是，黄色线代表罚函数的图像，绿色线代表原始损失函数的等高线，越靠近中心损失越小，即 $\tilde{w}^*$ 在绿色中心点，而越靠近黄线的点，损失则越大，但罚函数的值越小，最终在黄色点处有了**综合最优解**，也就是 $w^*$ 。

虽然在平衡点时，原始损失函数的最优解 $\tilde{w}^*$ 并非最小，但是不要忘了，我们的目的是防止过拟合，在加入罚函数的限制条件下，产生的最优解会比原先的损失函数小，这是因为 w 和罚函数成正比，w 越大，综合损失也就越大（比如原本 1 就是最优解，加上一个正的罚函数，最小值肯定比 1 小），其实也就是罚函数的功效，就是把偏大的权重拉进行分散，也就是减小，以此达到防止过拟合的作用。

补充一点，直观的感受就是罚函数让损失函数最优解不这么优，却更拟合！！过拟合的损失函数就是因为模型容量太大，把噪音的点都记住了才导致过拟合，正则化减小了模型容量（减小了参数范围）。

<img src="https://s2.loli.net/2024/06/03/Tb3nU9ohq5W7L8B.png" alt="image-20240603171637388" style="zoom:67%;" />

### 参数更新法则

- 计算梯度

  $\frac {\partial} {\partial w} (\ell(w,b) + {\frac \lambda 2}{ \sum_i{w_i}^2}) = \frac {\partial {\ell(w,b)}} {\partial w} + \lambda w$

- 更新参数，这里其实就更直观，相比原来的更新公式，后半部分没变，但更新前把 $w_t$ 乘上一个比 1 小的数，就是先把 w 变小再更新，就是先**衰退**在更新

  $w_{t+1} = (1-\eta \lambda)w_t - \eta\frac {\partial {\ell(w_t,b_t)}} {\partial w_t}$

- 通常 $\eta \lambda < 1$，称之为权重衰退