# 多层感知机  

## 简介

多层感知机（Multilayer Perceptron, MLP）是一种人工神经网络，包含多个层次的神经元（节点），用于处理复杂的非线性关系。它是深度学习和神经网络的重要组成部分，能够解决单层感知机无法处理的非线性问题。

比如在感知机不能解决的 XOR 问题，多层感知机通过加一层隐藏层解决，这里就不细说了。

## 原理

### 单隐藏层

单隐藏层的示意如下图，图中隐藏层只有一层，且输入层数据从 $h_1$ 依次输入到 $h_5$（仅下图）

<img src="https://s2.loli.net/2024/05/27/hjlSHoUtsJfF9eQ.png" alt="image-20240527000215117" style="zoom:67%;" />

#### 单隐藏层-单分类

- 输入 $x \in R^n $，这里的 $n$ 是输入的个数，即特征数
- 隐藏层 $W_1 \in R^{m \times n},b_1 \in R^m$，这里的 $m$ 是隐藏层的神经元个数，上图即 `m = 5`
- 输出层 $w_2 \in R^m,b_2 \in R$，由于是单分类，明显输出只有一个，也就是一个标量

公式如下：

- $h = \sigma(W_1x + b_1)$，这里的 $\sigma$ 是按元素的yuan激活函数（非线性），如 softmax，但其一般用在输出层而不是隐藏层。
- $o = w_2^Th + b_2$，$o$ 即输出，这里使用隐藏层的输出作为输出层的输入。

> 之所以激活函数是非线性的，是因为多层感知机的目的就是要解决非线性的问题，如果激活函数是线性的，那么按照以上公式（如这个单隐藏层问题），最终的输出 $o = w_2^TW_1x + b^`$ 依然是线性的。
>
> 那么无论有多少层隐藏层，最终都能用一个线性公式来表达，也就是无法解决非线性问题，也就背离了多层感知机的初衷。

#### 单隐藏层-多分类

输出公式：$y_1,y_2,...,y_k = softmax(o_1,o_2,...,o_k)$，这里指 k 类分类，softmax 函数作用于输出层而非隐藏层。

- 输入 $x \in R^n $
- 隐藏层 $W_1 \in R^{m \times n},b_1 \in R^m$
- 输出层 $W_2 \in R^{m \times k},b_2 \in R^k$，这里和上面的单分类不同，输出不是标量

公式如下：

- $h = \sigma(W_1x + b_1)$
- $o = W_2^Th + b_2$
- $y = softmax(o)$，这里指将隐藏层的输出作为输出层 softmax 函数的输入

### 多隐藏层

例如一个拥有三个隐藏层的感知机公式如下所示：

- $h_1 = \sigma(W_1x + b_1)$，特征作为输入
- $h_2 = \sigma(W_2h_1 + b_2)$，第一隐藏层的输出作为输入
- $h_3 = \sigma(W_3h_2 + b_3)$，第二隐藏层的输出作为输入
- $o = W_4h_3 + b_4$

<img src="https://s2.loli.net/2024/05/27/olSXyDGB5VzPbWN.png" alt="image-20240527162614038" style="zoom:67%;" />

这里的超参数是隐藏层数和每层隐藏层的大小，且一般从下到上每层隐藏层的大小依次减小，且按照一定规律减少，如倍数或 2 的指数数量（128，256，512）

>当模型比较复杂的时候，一般有两个思路设计感知机隐藏层：
>
>- 宽度：设置单隐藏层，将该隐藏层的大小设定较大
>- 深度：设置多隐藏层，将每个隐藏层的大小相对压缩的更小，从下到上依次递减（慢慢压缩），比如，如果输入是 m 个，那么第一层隐藏层可以设置 m + 1 个神经元，然后再慢慢减少

### 激活函数分类

#### Sigmoid 激活函数

$$
sigmoid(x) = \frac {1} {1 + exp(-x)}
$$

- 输出范围： (0, 1)

- 优点：输出值可以解释为概率

- 缺点：容易导致梯度消失，由于曲线平缓，其导数值接近 0，向上传递时梯度越来越小（待补充）`\\TODO`

#### Tanh 激活函数

$$
tanh(x) = \frac {1 - exp(-2x)} {1 + exp(-2x)}
$$

- 输出范围： (-1, 1)

- 优点：输出均值为0，收敛更快

- 缺点：与Sigmoid类似，也存在梯度消失问题

#### ReLU 激活函数

ReLU：rectified linear unit
$$
ReLu(x) = max(x, 0)
$$

- 输出范围： [0, ∞)

- 优点：计算简单，不会饱和（正区间），算起来很快，前几个激活函数要计算指数运算（比较贵）

- 缺点：可能导致“神经元死亡”，即某些神经元永远不会激活，就是有些特征为 0，导致对应的隐藏层神经元不会触发 

---

注：超参数是机器学习模型中设置并控制模型学习过程的参数，它们并不是通过模型训练过程自动学习到的，而是需要在训练前进行设定的。超参数的选择对模型的性能和训练效率有重要影响，其实就是手动设置的那部分参数（炼丹，调参）