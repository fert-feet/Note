# 丢弃法（dropout）

## 简介

丢弃法同权重衰退一样都是用于抑制过拟合的方法，是一种在神经网络中常用的正则化技术，其主要原理是在训练过程中，以一定的概率随机将神经网络中的部分神经元暂时“关闭”，即这些神经元的输出被设置为 0。这样可以防止模型过度依赖某些特定的神经元，增加了模型的泛化能力。

## 原理

### 动机

- 好的模型需要对输入数据的扰动进行鲁棒（robust，健壮性操作-泛化更好）
  - 使用有噪音的数据等价于 Tikhonov 正则（加入一个 L2 范数正则化项，来约束模型的复杂度）
  - 丢弃法：在层之间加入噪音

### 无偏差加入噪音

- 对 $x$ （输入数据）加入噪音得到 $x^{'}$，且希望 $E[x^{'}] = x$，即期望不变

- 丢弃法对每个元素做如下扰动，给定一个概率 p，使得一部分元素变成 0，另一部分放大，为什么要除 1-p，是因为这样才能保持 $E[x^{'}] = x$
  $$
  x_i^{'} = \begin{cases}
    0 & \text{with probablity p}\\
    \frac {x_i} {1-p} & \text{otherwise}
  \end{cases}
  $$

### 使用丢弃法

- 通常将丢弃法作用在隐藏层的输出上，公式如下，相当于将某些神经元置零（如下图）
  - $h = \sigma(W_1x + b_1)$，隐藏层输出
  - ${h}' = \text{dropout}(h)$，作用丢弃法
  - $o = W_2{h}' + b_2$，输出层的输出
  - $y = softmax(o)$，最终结果

![image-20240604162303465](https://s2.loli.net/2024/06/04/eWjSAZToPhUq2VN.png)

> 实际上相当于每次拿出一小部分神经元进行训练，最后在进行一个平均，其中丢弃的概率是一个超参数！！！

### 预测时的丢弃法

- 正则项只在训练中使用，他们影响模型参数的更新
- 预测过程是直接返回输出