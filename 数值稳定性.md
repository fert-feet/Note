# 数值稳定性+模型初始化+激活函数

## 神经网络的梯度

- 考虑 d 层的神经网络

  $h^t = f_t(h^{t-1})~\text{and}~y=\ell(f_d(...(f_1(x)))$（t 代表层次，h 代表输出）

- 计算损失的反向传播公式，由于向量对向量的求导是矩阵，因此可以将以下公式看成 d-t 次矩阵乘法

  $\frac {\partial \ell} {\partial W^t} = {\frac {\partial \ell} {\partial h^d}}{\frac {\partial h^d} {\partial h^{d-1}}}...{\frac {\partial h^{t+1}} {\partial h^t}}{\frac {\partial h^t} {\partial W^t}}$

这种大规模的乘法会导致两个结果：

- 梯度爆炸（梯度比 1 大）
- 梯度消失（梯度小于 1）

### 例子

- 加入如下 MLP（省略偏移）

   $f_t(h^{t-1}) = \sigma(W^th^{t-1})$

  ${\frac {\partial h^t} {\partial h^{t-1}}} = \text{diag}({\sigma}'(W^th^{t-1}))(W^t)^T$

- 当使用 ReLU 作为激活函数，那么 $\sigma({W^th^{t-1}})$ 不是 1 就是 0，这会导致最终的值是来自 $\prod_{i=t}^{d-1} (W^i)^T$，由于 w 的值大小不定，大于 1 时，此使 d-t 如果很大，值就会很大

### 梯度爆炸带来的问题

- 值超出值域，速度变慢
- 对学习率敏感
  - 学习率太大导致更大参数值，导致更大的参数，更加爆炸
  - 学习率太小，训练会直接无进展，稍大又爆炸
- 需要在训练过程不断调整学习率

### 梯度消失带来的问题

如果使用 sigmod 作为激活函数，意味着可能有 d-t 个小数值的乘积

- 梯度为 0，无法训练
- 对底层尤为严重-反向传播，底层最后计算，底层的梯度为0，导致底层训练比较不好

## 让训练更加稳定

- 目标是让梯度值在更合理的区间：如 [1e-6, 1e3]
- 将乘法变加法：ResNet，LSTM
- 归一化：梯度归一化，梯度裁剪
- 合理的权重初始和激活函数

### 让每层的方差是一个常数

- 将每层的输出和梯度都看作随机变量
- 让它们的均值和方差都保持一致

### 权重初始化

- 在合理的区间里随机生成初始参数
- 训练开始的时候更容易有数值不稳定
  - 离最优解远的地方梯度可能比较大
- 使用 $N(0,0.01)$ 初始化可能对小网络没问题，但不能保证深度神经网络也没问题

### Xavier 初始

- 经过一阵数学公式推导发现当满足 $n_{t-1}y_t = 1$ 和 $n_ty_t = 1$（$n_{t-1}$代表输入的维度，后者是输出的维度，$y_t$ 是层次的方差），可以分别保证每次前项的输出一致，后者保证梯度一样

- 但是两个条件难以同时满足（需要输入维度 = 输出维度）

- 解决方法：Xavier 使得 $y_t(n_{t-1} + n_t)/2=1$，其中 $y_t = 2/(n_{t-1} + n_t)$，其实就是变相折中
  - 正态分布 $N(0,\sqrt{2/(n_{t-1} + n_t)})$
  - 均匀分布 $\mu(-\sqrt{6/(n_{t-1} + n_t)},\sqrt{6/(n_{t-1} + n_t)})$，分布 $\mu[-a, a]的方差是$$a^2/3$，其实就满足方差的要求
- 如此可以适配输入和输出的维度

### 激活函数

经过数学推导，如果要均值为0，方差固定，激活函数必须是 $f(x) = x$

- 泰勒展开激活函数

  $\text{sigmoid(x)} = \frac 1 2 + \frac x 4 +...+O(x^5)$

  $\text{tanh(x)} = 0 + x +...+O(x^5)$

  $\text{relu(x)} = 0 + x for x>=0$

- 调整 sigmoid

  $\text{sigmoid-new} = 4 \times \text{sigmoid(x)} - 2$，以此达到 $f(x) = x$ 的目的，其他两个激活函数不用调整，在 0 点附近是满足条件的